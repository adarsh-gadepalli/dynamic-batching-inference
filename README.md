# Dynamic Batching Inference System

A high-performance inference server implementation using **FastAPI** and **PyTorch**, featuring a dynamic batching mechanism to optimize throughput and latency.

# How to Run
Open colab_benchmark.ipynb in Google Collab, select GPU runtime and run cells.
